\documentclass[]{elsarticle} %review=doublespace preprint=single 5p=2 column
\include{knitr_head.tex}
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}
\usepackage{lineno} % add
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{float}
\usepackage{float}
\usepackage{xcolor}
\usepackage{color}


% A modified page layout
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            pdfauthor={},
            pdftitle={Neuroconductor: A Framework for Reproducible Neuroimaging Analysis in R},
            colorlinks=true,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header


\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\emph{#1}}

\usepackage[nomarkers]{endfloat}
\newcommand{\fixme}[1]{{\color{red} #1}}
\begin{document}
\begin{frontmatter}

  \title{Neuroconductor: An R platform for Neuroimaging}
    \author[JHU]{John Muschelli\corref{c1}}
   \ead{jmusche1@jhu.edu} 
   \cortext[c1]{Corresponding Author}
    \author[Penn]{Jean-Philippe Fortin}
   \ead{fortin946@gmail.com} 
  
    \author[JHU]{Adrian Gherman}
   \ead{adig@jhu.edu} 
   \author[]{Brian Avants}
   \ead{stnava@gmail.com}
   
   \author[]{Brandon Whitcher}
   \ead{bwhitcher@gmail.com}   
   
    \author[JHU]{Brian S. Caffo}
   \ead{bcaffo@jhsph.edu} 
  
    \author[JHU]{Ciprian M. Crainiceanu}
   \ead{ccraini1@jhu.edu} 
  
      \address[JHU]{Johns Hopkins Bloomberg School of Public Health, Department of
Biostatistics, 615 N Wolfe St, Baltimore, MD, 21205}
    \address[Penn]{Perelman School of Medicine, University of Pennsylvania, Department of
Biostatistics and Epidemiology, 423 Guardian Drive, Philadelphia, PA
19104}
  
  \begin{abstract}
Neuroconductor \url{http://neuroconductor.org}  is an open-source platform for rapid testing and dissemination of reproducible computational imaging software. The goals of the project are to: 1) provide a centralized repository of R software dedicated to image analysis; 2) disseminate  software updates quickly; 3) train a large, diverse community of scientists using detailed tutorials and short courses; 4) increase software quality via automatic and manual quality controls; and 5) promote reproducibility of image data analysis. Based on the  programming language R (\url{https://cran.r-project.org/}), Neuroconductor starts with 24 inter-operable packages that cover multiple areas of imaging including visualization, data processing and storage, and statistical inference. Neuroconductor accepts new R package submissions, which are subject to a formal review and continuous automated testing. We provide a description of Neuroconductor's front- and back-end  and describe the user and developer experience.  
  \end{abstract}
 
 \end{frontmatter}

% Introduction
\section{Introduction}\label{introduction}

Neuroimaging software is heterogeneous, complex, and hard to use in fully reproducible analytic pipelines. These problems have been accentuated by the diversity of new imaging datasets and associated scientific problems. Indeed, many studies now collect data on thousands of subjects, at multiple visits, and different modalities. Storing, understanding, and analyzing such data is  daunting.  Neuroconductor will provide the software tools for using, improving and designing open source scripted software that depends on a minimum number of software platforms and is dedicated to improving the correctness, reproducibility, and speed of neuroimage data analysis. To achieve this, Neuroconductor will make no distinction between pre- and post-processing image analysis and will provide integrated data analytic approaches in R.


Neuroconductor provides data, methods, and software packages designed to support the analysis of image population studies in R, which allows integration with state-of-the-art statistical analysis software. It supports many types of imaging data including magnetic resonance imaging (MRI: structural, functional, and dynamic), computed tomography (CT), single-photon emission computed tomography, (SPECT), and magnetoencephalography (MEG). It is currently focused on human imaging, especially of the brain, but supports other biological imaging.  It can interface with mature platforms, such as Bioconductor \cite{bioc1,bioc2} and CRAN \cite{Hornik2016,r}. Neuroconductor can be used seamlessly for data fusion analyses where imaging, genomics, and other types of high-throughput data types are analyzed together with traditional measurements and health outcomes. In R these analyses can be integrated with well designed and tested analytic packages for visualization, inference, longitudinal and survival analysis, regression, network analysis, and machine learning.


\section{The user perspective}\label{section:user_perspective}
The Neuroconductor users interact with the platform via the Neuroconductor website \url{http://neuroconductor.org}. Users can install an R package, explore packages, or identify a workflow designed for their specific problem. If the workflow does not exist in Neuroconductor then users can create and submit their own workflow. 


\subsection{General description}\label{subsec:general_description}
To download a package an user would need to know the name of the package; this can be obtained from the \href{list of packages}{http://neuroconductor.org/list-current-packages} on the Neuroconductor webpage. For example, to download the packages \verb"fslr" and \verb"hcp" in R the command line instructions are 
\color{blue}
\begin{verbatim}
source("http://neuroconductor.org/neurocLite.R") 
neuro_install(c("fslr", "hcp"))
\end{verbatim}
\color{black}

Each package contains documentation as vignettes and manuals. Large areas of research covering multiple package combinations are described under Training. Some of these areas contain tutorials and Massive  Open Online Courses (MOOCs) that provide a rapid introduction of more complex concepts and workflows. Examples of such courses are Principles of fMRI  \url{https://www.coursera.org/learn/functional-mri} and Introduction to Neurohacking in R \url{https://www.coursera.org/learn/neurohacking}.  If the user does not find what they are looking for then they can develop their own tutorial and/or course that can be submitted to Neuroconductor. Furthermore, the user can turn to the Support forum,  can attend short courses, take free online MOOCs, or read tutorials. All package developers are users and some users become package developers.

The user may have their own data, use a data package from Neuroconductor, and/or use a Neuroconductor package  to download data from an internet repository.  The first step is to manipulate, visualize, and quality control the data.  Most quality controls can be performed at the image or subject level, though the distribution of quality metrics in the population can be used, as well; see Mejia and al. \citep{mejia2015pca} for an example. Once data quality has been assessed, subjects can be excluded or set aside for subsequent analyses.   

A major advantage of the R environment is that analyses can take advantage of well tested,  state-of-the-art statistical packages. For example, using demographic information,  regression, mixed effects models, or survival analysis is straightforward in R.  Results containing spatial information can be mapped back to an image, while other results can be displayed using plotting packages such as ggplot2 \citep{ggplot2}.  All these steps can be wrapped in a series of reproducible reports using \verb"knitr" and \verb"rmarkdown" \citep{rmarkdown,knitr}. Another advantage of R is that complex processing steps and complete analyses can be wrapped into R packages. For example, the \verb"ichseg" \cite{ichseg,muschelli2016pitch} \verb"R" package is designed for intra-cranial hemorrhage segmentation and it is a wrapper for multiple functions. It uses a  single CT scan as input, processes the data using FSL \citep{fsl}, called through \texttt{fslr} \citep{fslr}, and creates a segmentation of the hemorrhagic stroke based on a pre-trained supervised machine learning algorithm. Last, packages can be deployed as Shiny applications \citep{shiny}, which are web applications built in R and can be used with internet browsers.  Shiny applications can be hosted on a custom or a Shiny server, such as \url{www.shinyapps.io}. Below we provide several case studies to build up the intuition for interfacing with  Neuroconductor.

% Case study
\subsection{Case study: structural magnetic resonance imaging processing.} Figure~\ref{fig:flow} provides an example of a typical processing pipeline for multi-sequence MRI data. Analysis of such data often begins with a conversion of Digital Imaging and Communications in Medicine (DICOM) files, which are essentially JPEG images with header information.  These DICOM files are converted to the Neuroimaging Informatics Technology Initiative (NIfTI) format, one of the most common neuroimaging formats. This process can be done in \verb"R" using the \pkg{dcm2niir} (\url{https://www.nitrc.org/projects/mricrogl/}) or \pkg{oro.dicom} \cite{dicom_nifti} packages. The resulting NIfTI files can be converted to array-based S4 objects  using the \pkg{oro.nifti} \cite{oro.nifti} package or objects based on C++ pointers using the \pkg{ANTsR} \cite{antsr} and \pkg{RNifti} \cite{Rnifti} packages.  These packages also provide the ability to read other neuroimaging formats.

\begin{figure}[!ht]
  \begin{center}
    \caption{Typical processing pipeline for multi-sequence structural MRI data.}
    \includegraphics[height=6.5in]{Imaging_Pipeline_Flowchart.pdf}
    \label{fig:flow}
  \end{center}
\end{figure}


After conversion, image intensity inhomogeneity correction can be applied to ensure that each tissue has similar intensity distributions across locations in the brain (e.g. top versus bottom of the brain). Image inhomogeneities are typically caused by magnetic field inhomogeneities and can be handled by multiple methods. Neuroconductor currently contains three such methods implemented in FSL  via \pkg{fslr} \cite{zhang_segmentation_2001},  \pkg{freesurfer} \cite{sled_nonparametric_1998}, \pkg{ANTsR} \cite{sled_nonparametric_1998}, or N4 \cite{tustison_n4itk:_2010}.  The next step is co-registration, the process that spatially realigns all images in the sequence to one of the images in the sequence, usually a T1-weighted image.  Co-registration can be done using \code{flirt} (\pkg{fslr}), \code{antsRegistration} (\pkg{ANTsR}), or \code{dramms} (\pkg{drammsr}). Co-registration is followed by brain extraction, also known as skull stripping.  This procedure is commonly done on the T1-weighted image, and can be done using \pkg{spm12r}, \pkg{fslr}, \pkg{freesurfer}, or \pkg{ANTsR}. Brain extraction can be followed by intensity normalization,  which transform the arbitrary units in MRI into interpretable units across subjects. This is not usually thought of as a standard pre-processing step, but it is crucial in many applications. For example, one could be interested in subtracting two images that were collected longitudinally to identify changes or one may want to investigate changes in voxel or ROI intensities over time.  This can be done using z-scoring with respect to a particular tissue class as implemented in WhiteStripe (\pkg{WhiteStripe}) \citep{whitestripe}, standard and robust z-scoring relative to whole brain (\pkg{neurobase}), histogram matching  (implemented in \pkg{RAVEL}), or removal of unwanted variation \citep{ravel} (\pkg{RAVEL}).  Further subject-specific and population analyses may require registration to a population-level template and/or tissue class segmentation. Both these approaches can be done using Neurocondoctor packages including \pkg{spm12r}, \pkg{fslr}, and \pkg{ANTsR}.

While the flowchart in Figure~\ref{fig:flow} provides a conceptual pre-processing pipeline, deploying an explicit and reproducible pipeline requires specific choices at every step that may depend on multiple tuning parameters. In \verb"R" one can be explicit about these choices, provide a software suite of packages and tuning parameters, and quickly compare results based on different combinations of software and platforms.  

% Example of a cross-package Neuroconductor workflow: preprocessing and analysis of diffusion tensor imaging data
\section{Example of a cross-package Neuroconductor workflow: preprocessing and analysis of diffusion tensor imaging data}

In this section, we discuss an example of an entire pipeline (preprocessing and statistical analysis) performed entirely with Neuroconductor.  We start with a simple question: in a population of healthy subjects, is there any difference in the white matter (WM) microstructure between males and females? Diffusion tensor imaging (DTI) has been  used extensively to study WM fiber structure. DTI  takes advantage of the differential water diffusivity in the WM tracts relative to other brain structures. Fractional anisotropy (FA) and mean diffusivity (MD) are two scalar maps commonly derived from DTI images to study the diffusivity properties of the brain. 

To investigate potential gender differences between white WM fiber tracts, we use DTI images from healthy young adults from The Human Connectome Project (HCP) available at \url{db.humanconnectome.org}.  HCP includes a large cohort of individuals ($N > 1200$) with a vast amount of neuroimaging data, including structural magnetic resonance imaging (sMRI), task and resting state functional MRI (fMRI) and diffusion tensor imaging (DTI).

% Step 1: Downloading data from the hcp package
\subsubsection{Step 1: Downloading data using the \pkg{hcp} package}
The first step is to download the minimally preprocessed DTI data \citep{hcpminimal} and structural T1w MRI images, available  for 781 subjects (436 females and 345 males) using the \texttt{hcp} \citep{hcp} package.  The \pkg{hcp} package is an R interface for downloading data from the HCP database, which is publicly available from Amazon Web Services (AWS). A description of how to connect to the HCP database via AWS can be found \href{https://wiki.humanconnectome.org/display/PublicData/How+To+Connect+to+Connectome+Data+via+AWS}{here}.  After accepting the data use terms, one needs to obtain AWS credentials, which include an access key ID and a secret  key. The \texttt{hcp} package in \texttt{R} accesses the data from an Amazon Simple Storage Solution (S3) bucket using these two AWS access keys:

\color{blue}
\begin{verbatim}
library(hcp)
set_aws_api_key(access_key = "ACCESS_KEY", secret_key = "SECRET_KEY")
\end{verbatim}
\color{black}
For instance, the complete diffusion data directory for subject 100307 can be downloaded using the \verb"download_hcp_dir" function in the \pkg{hcp} package:
\color{blue}
\begin{verbatim}
result = download_hcp_dir("HCP/100307/T1w/Diffusion")
\end{verbatim}
\color{black}
The \texttt{result} is an \texttt{R} list containing the file names, the directory, and the \texttt{http} request that was sent to the Amazon S3 bucket. Data from other subjects can be downloaded similarly. After the minimally processed DTI data are downloaded it can be further processed using \pkg{rcamino}. 

% Step 2: Processing data with rcamino
\subsubsection{Step 2: Processing DTI data with rcamino}

We preprocess the DTI data using the Neuroconductor R package \texttt{rcamino}, an R interface for the open-source DTI software \texttt{Camino} \citep{camino}. The package is used to create and fit the diffusion tensor models, and generate the fractional anisotropy (FA) and mean diffusivity (MD) maps. We show below the associated \texttt{R} code. 

\color{blue}
\begin{verbatim}
# Extract the DTI files for the subject and name them using the NIfTI format
library(neurobase)
library(rcamino)
outfiles = result$output_files
names(outfiles) = neurobase::nii.stub(basename(outfiles))

# Specify the B-values and B-vectors used in the HCP database for further processing:
camino_fsl2scheme(bvecs = outfiles[["bvecs"]], bvals = outfiles[["bvals"]],
    outfile = "hcp.scheme")

# Convert the diffusion data from NIfTI to Camino format:
camino_image2voxel(infile = outfiles[["data"]], outfile = "dwi.Bfloat")

# Fit the diffusion tensor imaging model:
camino_modelfit(infile = "dwi.Bfloat", outfile = "dt.Bdouble", 
    scheme = "hcp.scheme", gradadj = outfiles[["grad_dev"]],
    model = "ldt", mask = outfiles[["nodif_brain_mask"]])

# Produce the FA and MD maps from the fitted tensor data:
fa = camino_fa_img(infile = "dt.Bdouble", inputmodel = "dt", header = outfiles[["data"]])
md = camino_md_img(infile = "dt.Bdouble", inputmodel = "dt",header = outfiles[["data"]])
\end{verbatim}
\color{black}
Note that \texttt{Camino} requires the B-values and B-vectors of the DTI to conduct the DTI model fit. The B-values are the amount of diffusion weighting used for each volume. The B-vectors are the gradient directions used by the scanner that may depend on the number of directions; for more details, see \url{http://www.diffusion-imaging.com/2014/03/dti-quality-control-part-2-tensor.html}. The process is then repeated for all subjects with available DTI data. The process can be sped up by  parallel computing.

% Step 3: joint analysis
\subsubsection{Step 3: Nonlinear registration to template} 
The next step is to prepare the data for voxel-wise analysis, which requires the FA and MD maps to be spatially registered to a common template. For each subject, we use the \texttt{download\_hcp\_file} function from the \texttt{hcp} package to download the T1w image with extra-cranial voxels removed. We then use the diffeomorphic non-linear registration implemented in \texttt{ANTsR}, wrapped in \texttt{extrantsr}, to register FA and MD maps to the 1mm isotropic Eve template T1w image \citep{eve}. The Eve template is a single-subject template created by the Laboratory of Brain Anatomical MRI led by Professor Susumu Mori at Johns Hopkins University. The Eve template is made available in the Neuroconductor data package \texttt{EveTemplate}. Alternatively, one could register  images to the MNI template, which is available in the Neuroconductor data package \texttt{MNITemplate}.   Each registered DTI map is saved as a standard NIfTI file for further analyses. The \texttt{R} code is presented below.

\color{blue}
\begin{verbatim}
# Specify the path of the Eve template file:
library(EveTemplate)
eve_template = getEvePath()

# Perform non-linear registration usin Syn in ANTsR:
t1_file = download_hcp_file("HCP/100307/T1w/T1w_acpc_dc_restore_brain.nii.gz")
reg = extrantsr::registration(filename = t1_file,  template.file = eve_template, 
    typeofTransform = "SyN", interpolator = "Linear", remove.warp = FALSE)

# Create the registered FA and MD maps to the Eve atlas:
fa_eve = ants_apply_transforms(fixed = eve_template, moving = fa, 
    transformlist = reg$fwdtransforms)
md_eve = ants_apply_transforms(fixed = eve_template, moving = md,
    transformlist = reg$fwdtransforms)    
\end{verbatim}
\color{black}
The results of these processing steps are $3$D arrays containing the FA and MD maps registered to the Eve atlas for every subject. While at this point we do not conduct region of interest (ROI) analyses, the template would be very useful to extract subject-specific ROIs. An ROI could be defined as an anatomical region, regions obtained from other analyses, or regions that are manually delineated. Here we focus on voxel-wise analysis across subjects, which can be done because images are registered to the same template.

% Step 4: statistical analysis
\subsubsection{Step 4: Statistical analysis} 
In general, we consider that principled data analysis is Statistical analysis. However, to keep with Neuroimaging phraseology, here we refer to statistical analysis as the analysis of populations of images.

We first read the registered NIfTI files into R and create a matrix of voxels intensities with voxels as rows, and subjects as columns. We create one matrix for the FA maps, and one for the MD maps, using the  function \texttt{nifti2matrix} from the package \texttt{neurobase}. In this example, each matrix has $1,372,619$ rows (number of voxels in the MNI template) and $781$ columns (number of subjects in the dataset). 

There are many options in R to quantify the association of the FA and MD intensities with gender. The simplest approach is to calculate mass-univariate two-sample t-statistics (t-statistics computed at each voxel separately), which can be quickly computed using \texttt{limma}, a popular R package from the Bioconductor project \url{https://www.bioconductor.org/}. The package was originally developed for the analysis of high-throughput genomic data \citep{ritchie2015limma}, but many of the package functionalities can  be used in neuroimaging applications without any additional effort. The \texttt{limma} package also implements Empirical-Bayes (EB) methods to estimate  t-statistics based on variance shrinkage (called moderated t-statistics), which has been shown to performe well for small sample-size studies \citep{berkeley2004linear,ritchie2015limma}. The Neuroconductor package \texttt{RAVEL} contains the convenience function \texttt{getTMap}, which calculates t-statistics based on the \texttt{limma} package.  The \texttt{R} code is presented below.

\color{blue}
\begin{verbatim}
JP provides code here
# nifti2matrix
\end{verbatim}
\color{black}

% Step 5: Visualization of the results
\subsubsection{Step 5: Visualization of the results}
\color{blue}
\begin{verbatim}
JP provides code and images here
# 
\end{verbatim}
\color{black}

% Other analyses
\subsection{Advanced data analysis and visualization}
An important advantage  of the  \texttt{R} environment  is that  more sophisticated voxel-level analyses  can easily be  implemented.  For  example, one could be interested  whether  there  are  additional confounders  for the association between  FA and gender or  whether  image intensities predict health outcomes. The \texttt{lm} and \texttt{glm} packages in \texttt{R} are designed specifically to address such questions. If images are observed longitudinally or they are used as baseline predictors in longitudinal studies one could use the mixed effects \texttt{gee} \cite{gee} and \texttt{lme4} \cite{lme4} packages. If one is interested in modeling survival time based on baseline images then the \texttt{survival} package \cite{survival-package,survival-book} can be used.

Another  advantage  of the close integration between pre- and post-processing tools is that results can easily be mapped back in the native space or on the template. In particular, this can help localize significant associations by using the available label templates. For example, in our analysis of white matter by gender we could identify the voxels where the largest differences between genders are identified.  Below we provide the  \texttt{R} code  for visualization  and  localization of these results

\color{blue}
\begin{verbatim}
JP provides code and images here
# 
\end{verbatim}
\color{black}

\section{Large multi-site neuroimaging data fusion}\label{sec:datafusion}
Short section about how R can be used to statistically combine large datasets using the \texttt{sva} package \citep{svapackage} that implements the SVA and ComBat methods \citep{combat}. Talk about RUV and RAVEL \citep{ravel}. 

\section{The developer perspective}\label{section:dev_perspective}
To submit a package, the author/maintainer of the package will have to provide their name, a valid email address and the Github URL for the package. A short description can also be added but it is not mandatory. Once the package is submitted several initial checks will be conducted; see Figure~\ref{fig:stage1} for details. The checks verify that this is a valid package with an existing DESCRIPTION file containing metadata about the package. If the initial tests are successful, an email with a randomly generated \texttt{URL} will be sent to the package maintainer to verify the submission and prevent accidental/spam submissions. If the maintainer does not verify the package in a 24 hour interval, the package will be marked for deletion. 

Once the verification is complete, the package will be processed according to the workflow described in Figure~\ref{fig:stage2}. Neuroconductor will check if this is a new package or a package update request and will either clone the package (new) or do a pull from the package GitHub (existing). The DESCRIPTION file and the .travis.yml files will be changed to reflect the Neuroconductor status for this package (\url{http://neuroconductor.org/help-package-changes} contains the complete list of changes):
\begin{enumerate}
\item original - this is a identical copy of the submitted package.
\item stable - the DESCRIPTION file and .travis.yml are modified to take into account already existing Neuroconductor stable packages.
\item development - similar to the stable, but for the existing Neuroconductor development packages.
\end{enumerate}
Next, the package is pushed to the central Neuroconductor GitHub [\url{https://github.com/neuroconductor}] and submitted to Travis CI [\url{https://travis-ci.org/}] to be built and checked on multiple systems. The author of the package receives an automatic email indicating whether the package was built successfully and is integrated with Neuroconductor together with a description a file containing pertinent information about the process. The package author can ask for help from the maintainers of Neuroconductor [\url{http://neuroconductor.org/contact-us}] both for compatibility issues as well as for R-specific questions. 




\section{Neuroconductor infrastructure}\label{sec:backend}
The Neuroconductor project is hosted on a Red Hat Enterprise Linux Server (version 6.8 Santiago) located within the Johns Hopkins School of Public Health data center. The project architecture is based on the LAMP (\textbf{L}inux / \textbf{A}pache / \textbf{M}ySQL / \textbf{P}HP) stack, providing a flexible, fully customizable and secure end product. We are using Drupal, a CMS (\textbf{C}ontent \textbf{M}anagement \textbf{S}ystem) to provide the user interface to the Neuroconductor module. 

\begin{figure}[!ht]
  \begin{center}
    \caption{Neuroconductor initial package processing. This flowchart depicts the initial package submission stage. If the DESCRIPTION file is present it will parsed and the submitted version of the package is checked against existing Neuroconductor packages. If this version is not already in Neuroconductor the email based verification process is started. Once the maintainer verifies the submission the package is ready to be tested.}
    \label{fig:stage1}
    \includegraphics[height=0.9\textheight]{flow_stage1_draft.png}\label{fig:package_lifetime_1}
% * <adig@jhu.edu> 2016-11-30T18:19:59.969Z:
%
% ^.
  \end{center}
\end{figure}
\begin{figure}[!ht]
  \begin{center}
    \caption{Neuroconductor package code testing. The package is cloned/updated on the Neuroconductor server and Travis CI checks are initiated for the original version of the package together with a stable and development version of this package. The DESCRIPTION and travis.yml files are updated to reflect the stable/development package status. Once the Travis CI checks are done the package listing is updated to reflect the check results.}
    \label{fig:stage2}
    \includegraphics[height=0.9\textheight]{flow_stage2_draft.png}\label{fig:package_lifetime_2}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table 
\begin{table}[!ht]
%\scriptsize
\centering
\caption{\textbf{Current R packages available on Neuroconductor}}\label{tab:packages}
\begin{tabular}{lll}
\hline \\[-2ex]
\textbf{Package} & \textbf{Description}   & \textbf{References} \\
\hline \\ [-1.5ex]
\multicolumn{3}{l}{\textbf{Software packages}}\\
\\ [-1.5ex]
\texttt{neurobase} & Base functions for Neuroconductor  &   \\
\texttt{oro.nifti} & Description  &  \citep{oro} \\
\texttt{oro.dicom} & Description  &  \citep{oro} \\
\texttt{oro.asl} & Description  &   \\
\texttt{oro.pet} & Description  &   \\
\texttt{ANTsR} & Advanced Normalization Tools  & \citep{ants} \\
\texttt{extrantsr} & Extensions for ANTsR  &  \\
\texttt{fslr} & R package for FSL  & \citep{fslr,fsl} \\
\texttt{freesurfer} & R package for FreeSurfer  & \citep{freesurfer} \\
\texttt{oasis} & OASIS lesion segmentation & \citep{oasis}\\
\texttt{WhiteStripe} & White Stripe intensity normalization  & \citep{whitestripe} \\
\texttt{RAVEL} & Statistical analysis of structural MRIs  & \citep{ravel} \\
\texttt{hcp} & R interface for the Human Connectome Project database &  \\
\hline \\ [-1.5ex]
\multicolumn{3}{l}{\textbf{Data packages}}\\
\\ [-1.5ex]
\texttt{kirby21.t1} &   & \citep{kirby}  \\
\texttt{kirby21.t2} &   &  \citep{kirby} \\
\texttt{kirby21.flair} &   &  \citep{kirby} \\
\texttt{kirby21.dti} &   &  \citep{kirby} \\
\texttt{kirby21.fmri} &   &  \citep{kirby} \\
\texttt{kirby21.mt} &   &  \citep{kirby} \\
\texttt{kirby21.vaso} &   &  \citep{kirby} \\
\texttt{kirby21.asl} &   &  \citep{kirby} \\
\hline \\ [-1.5ex]
\multicolumn{3}{l}{\textbf{Template packages}}\\
\\ [-1.5ex]
\texttt{MNITemplate} &   &   \\
\texttt{EveTemplate} & Eve Atlas and White Matter parcellation map  & \citep{eve}  \\
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Why R?}\label{why-r}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Leverage R resources/plotting/reproducibility/package development
\item
  Leverage Bioconductor resources - they have similar problems, years of
  testing, and code
\item
  Bayesian Statistics
\end{enumerate}

\subsubsection{Third Party Software}\label{third-party-software}

FSL \citep{fsl}, AFNI \citep{afni}, FreeSurfer \citep{freesurfer}, SPM \citep{spm}


We will refer to R-Forge, OmegaHat, and CRAN as standard
repositories.



\subsection{Data Packages}\label{data-packages}

Like Bioconductor, we need data packages that allow users to test
software and examples on.

\bibliographystyle{plainnat}
\bibliography{Neuroconductor}

%\section*{References}\label{references}
%\addcontentsline{toc}{section}{References}
%
%\hypertarget{refs}{}
%\hypertarget{ref-carpux5fsecretux5f2012}{}
%Carp, Joshua. 2012. ``The Secret Lives of Experiments: Methods Reporting
%in the fMRI Literature.'' \emph{NeuroImage} 63 (1): 289--300.
%doi:\href{https://doi.org/10.1016/j.neuroimage.2012.07.004}{10.1016/j.neuroimage.2012.07.004}.
%
%\hypertarget{ref-gentleman2004bioconductor}{}
%Gentleman, Robert C, Vincent J Carey, Douglas M Bates, Ben Bolstad,
%Marcel Dettling, Sandrine Dudoit, Byron Ellis, et al. 2004.
%``Bioconductor: Open Software Development for Computational Biology and
%Bioinformatics.'' \emph{Genome Biology} 5 (10). BioMed Central: 1.

\end{document}


